<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
  <title>Hongtao Wen (文洪涛)</title>

  <meta charset="utf-8">
  <meta name="robots" content="index, follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta name="keywords" content="Hongtao Wen, 文洪涛, computer vision, deep learning, Dalian University of Technology">
  <meta name="author" content="Hongtao Wen">

  <link rel="stylesheet" href="./stylesheet.css" type="text/css" />
  <link rel="shortcut icon" href="images/favicon_wht.png" type="image/x-icon" />

  <script src="jquery.min.js"></script>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./"><img src="images/HWen_smile.png" alt="" height="222px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"><font size="4">Hongtao Wen (</font><font size="4"; font style="font-family:Microsoft YaHei">文洪涛</font><font size="4">)</font></a><br />
<i> Master's Student </i>
<br /><br />
<a href="http://ice.dlut.edu.cn/Home.htm" target="_blank">School of Information and Communication Engineering</a>, <a href="https://en.dlut.edu.cn//" target="_blank">Dalian University of Technology</a><br />
<br />
Location: No.2 Linggong Road, Ganjingzi District, Dalian City, Liaoning Province, China<br />
<class="staffshortcut">
 <A HREF="#Interest">Research Interest</A> | 
 <A HREF="#Education and Intern">Education and Intern</A> | 
 <A HREF="#Publications">Publications</A> | 
 <A HREF="#Projects">Projects</A> | 
 <A HREF="#Skills">Skills</A> | 
 <A HREF="#Honors & Awards">Honors & Awards</A> | 
 <A HREF="#Misc">Misc</A>
<br />
<br />

Email: <a href="mailto:ht.wen@outlook.com" style="color: black;">ht.wen@outlook.com</a> (primary)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="mailto:wenht@mail.dlut.edu.cn" style="color: black;">wenht@mail.dlut.edu.cn</a>
<br />

[<a href="./files/CV_HWen.pdf" target="_blank">CV_HWen.pdf</a>]
[<a href="https://github.com/hatimwen" target="_blank">GitHub</a>]
[<a href="https://scholar.google.com/citations?user=w65pufEAAAAJ" target="_blank">Google Scholar</a>]
<!-- [<a href="https://www.linkedin.com/in/hongtao-wen-222b51141/" target="_blank">LinkdeIn</a>] -->
[<a href="https://blog.csdn.net/Hatim98/" target="_blank">CSDN</a>]
[<a href="./images/Wechat.jpeg" target="_blank">Wechat</a>]
</td></tr></table>


 
<A NAME="Interest"><h2>Research Interest</h2></A>
I work in the field of Robotics, Computer Vision and Deep Learning.
<br />
Currently, I focus on the following research topics:
<ul>
  <li>Robotic Perception</li>
  <li>Category-Level 6D Object Pose Estimation</li>
  <li>6DoF Grasp Pose Estimation</li>
</ul>


 
<A NAME="Education and Intern"><h2>Education and Intern</h2></A>
<ul>
  <li>2022.3-2022.6 &nbsp;&nbsp;&nbsp;&nbsp;
    AI Research Intern in Gongyuan Sanqian Technology Co., Ltd.
  </li>
  <li>2020.9-NOW &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M.E. in
    <a href="http://ice.dlut.edu.cn/Home.htm" target="_blank" style="color: black;">SICE</a>, <a href="https://en.dlut.edu.cn//" target="_blank" style="color: black;">Dalian University of Technology</a>.
    Academic GPA 87.6/100
  </li>
  <li>2016.9-2020.6 &nbsp;&nbsp;&nbsp;&nbsp; B.E. in
    <a href="http://ice.dlut.edu.cn/Home.htm" target="_blank" style="color: black;">SICE</a>, <a href="https://en.dlut.edu.cn//" target="_blank" style="color: black;">Dalian University of Technology</a>.
    GPA 87.3/100, top 10%
  </li>
</ul>


<A NAME="Publications"><h2>Publications</h2></A>

<p><b>Conferences</b>: </p>
<font size="3">

  <table class="imgtable"><tbody>
    <tr>
      <td>
        <img src='images/TransGrasp_before.png' width="160">
      </td>
      <td align="left">
        <br>
        TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance<br>
        <strong>Hongtao Wen*</strong>, 
        <a href="https://github.com/yanjh97" style="color: black;">Jianhang Yan*</a>,
        <a href="https://github.com/swords123" style="color: black;">Wanli Peng</a>,
        <a href="http://faculty.dlut.edu.cn/1989011006/en/index.htm" style="color: black;">Yi Sun</a>
        <br>
        <i>European Conference on Computer Vision 2022 (<b>ECCV 2022</b>)</i>, * indicates equal contribution.
        <br>
        <!-- <p style="color:red"><b>PDF and codes are coming soon!</b></p> -->
        <a href="https://sites.google.com/view/transgrasp">Webpage</a> /
        <a href="https://github.com/yanjh97/TransGrasp">Code</a> /
        <a href="https://arxiv.org/pdf/2207.07861.pdf">PDF</a>
        <br>
        <br>
        <p>
          We propose TransGrasp, a category-level grasp pose estimation method that predicts grasp poses of a category of objects by labeling only one object instance.
        </p>
      </td>
    </tr>

    <tr onmouseout="SSC_6D_stop()" onmouseover="SSC_6D_start()">
      <td>
        <div class="one">
          <div class="two" id='SSC_6D_image'>
            <img src='images/SSC-6D_after.jpg' width="160">
          </div>
            <img src='images/SSC-6D_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function SSC_6D_start() {
            document.getElementById('SSC_6D_image').style.opacity = "1";
          }

          function SSC_6D_stop() {
            document.getElementById('SSC_6D_image').style.opacity = "0";
          }
          SSC_6D_stop()
        </script>
      </td>
      <td align="left">
        <br>
        Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation
        <br>
        <a href="https://github.com/swords123" style="color: black;">Wanli Peng</a>,
        <a href="https://github.com/yanjh97" style="color: black;">Jianhang Yan</a>,
        <strong>Hongtao Wen</strong>, 
        <a href="http://faculty.dlut.edu.cn/1989011006/en/index.htm" style="color: black;">Yi Sun</a>
        <br>
        <i>Thirty-sixth AAAI Conference on Artificial Intelligence (<b>AAAI 2022</b>)</i>
        <br>
        <a href="https://swords123.github.io/pages/ssc-6d/page">Webpage</a> /
        <a href="https://aaai-2022.virtualchair.net/poster_aaai3056">Introduction Video</a> /
        <a href="https://github.com/swords123/SSC-6D/">Code</a> /
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20104/19863">PDF</a>
        <br>
        <br>
        <p>
          We present a self-supervised method for category-level 6D pose estimation, SSC-6D, which can predict unseen object poses without explicit pose annotations and exact 3D models in real scenarios for training.
        </p>
      </td>
    </tr>
  </tbody></table>


</font>

<br />
<p><b>Patents</b>: </p>
<font size="3">
  <ul>
  
  <li>Updaing ...</li>
  </ul>

</font>


<A NAME="Projects"><h2>Projects</h2></A> 
 
<p><b>Research Projects</b>: </p>
<font size="3">
  &nbsp;&nbsp;&nbsp;&nbsp; These videos demonstrate some specific cases where TransGrasp predicts robust grasp poses for robotic manipulation.
<table border=none>
  <tr>
    <td>
      <object width="290" height="160">
        <param name="movie" value="https://player.bilibili.com/player.html?aid=809360417&;bvid=BV1A34y1k74Q&cid=516321808&page=1&as_wide=1&high_quality=1&danmaku=0"></param>
        <param name="allowFullScreen" value="true"></param>
        <embed src="https://player.bilibili.com/player.html?aid=809360417&;bvid=BV1A34y1k74Q&cid=516321808&page=1&as_wide=1&high_quality=1&danmaku=0" allowfullscreen="true" width="290" height="160">
        </embed>
        </object>
    </td>
    <td>
        <object width="290" height="160">
          <param name="movie" value="https://player.bilibili.com/player.html?aid=636823972&bvid=BV1wb4y1x7R2&cid=516322588&page=1&as_wide=1&high_quality=1&danmaku=0"></param>
          <param name="allowFullScreen" value="true"></param>
          <embed src="https://player.bilibili.com/player.html?aid=636823972&bvid=BV1wb4y1x7R2&cid=516322588&page=1&as_wide=1&high_quality=1&danmaku=0" allowfullscreen="true" width="290" height="160">
          </embed>
          </object>
    </td>
    <td>
        <object width="290" height="160">
          <param name="movie" value="https://player.bilibili.com/player.html?aid=979297739&bvid=BV1Y44y1n7g1&cid=516319371&page=1&as_wide=1&high_quality=1&danmaku=0"></param>
          <param name="allowFullScreen" value="true"></param>
          <embed src="https://player.bilibili.com/player.html?aid=979297739&bvid=BV1Y44y1n7g1&cid=516319371&page=1&as_wide=1&high_quality=1&danmaku=0" allowfullscreen="true" width="290" height="160">
          </embed>
          </object>
    </td>
  </tr>
  <tr>
    <td>
      <strong>Robot Demonstration</strong>: robot-assisted watering mobility-impaired individuals.
    </td>
    <td>
      <strong>Robot Demonstration</strong>: autonomously pouring water from cup into bowl.
    <!-- </br><i>(For clarity, melon seeds are substitute.)</i> -->
    </td>
    <td>
      <strong>Robot Demonstration</strong>: autonomously grasping household objects.
    </td>
  </tr>
</table>

</font>

<br />
<p><b>Open-Source Projects</b>: </p>
<font size="3"> 
<ul>
  <li>
    <a href="https://github.com/GeorgeDu/vision-based-robotic-grasping">Vision-Based Robotic Grasping</a>:
    A continuously updated summary of papers related to vision-based grasping, where I help to write a convenient program to automatically download these papers without human interference.
  </li>
  <li>
    <a href="https://github.com/hatimwen/Paddle_VIT_tutorial/blob/main/README_en.md">Paddle ViT Tutorial</a>:
    The repository provides demonstration codes for researchers to implement Vision Transformer(ViT) using an industrial deep learning framework called PaddlePaddle step by step.
  </li>
  <li>
    <a href="https://github.com/hatimwen/paddle_pit/blob/main/README_en.md">Paddle PiT</a>:
    An unofficial repository implemented by PaddlePaddle of <i>Spatial Dimensions of Vision Transformers</i>, which is a Pooling-based Vision Transformer (PiT).
  </li>
  <li>
    <a href="https://github.com/hatimwen/paddle_greedyhash/blob/main/README_en.md">Paddle GreedyHash</a>:
    An unofficial repository implemented by PaddlePaddle of <i>Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN</i>, which aims to tackle the NP hard problem in Deep Hashing. 
  </li>
  <li>
    <a href="https://github.com/hatimwen/paddle_hashnet/blob/main/README_en.md">Paddle HashNet</a>:
    An unofficial repository implemented by PaddlePaddle of <i>HashNet: Deep Learning to Hash by Continuation</i>, which is a novel deep architecture for deep learning to hash by continuation method with convergence guarantees.
  </li>
<li>More open-source contents can be found on <a href= "https://github.com/hatimwen?tab=repositories" target="_blank">GitHub</a> and <a href= "https://aistudio.baidu.com/aistudio/personalcenter/thirdview/198330/" target="_blank">百度 AI Studio</a>.
</li>
</ul>
</font>

<A NAME="Skills"><h2>Skills</h2></A>
<font size="3"> 
<ul>
  <li>Excellent in project analysis, implementation and coordination.</li>
  <li>Language: Chinese (native proficiency) & English (proficiency, IELTS score 7.0).</li>
  <li>Programming language: Python, C++, C.</li>
  <li>In-depth knowledge of Robotics, Computer Vision and Deep Learning.</li>
  <li>Familiar with deep learning frameworks: PyTorch, PaddlePaddle.</li>
  <li>Familiar with the programming of robot ROS system.</li>
  <li>Self-motivated with passion for researches and technologies.</li>
</ul>
</font>

<A NAME="Honors & Awards"><h2>Honors & Awards</h2></A>
<font size="3"> 
<ul>
  <li>2022, Outstanding Graduate Student of DLUT.</li>
  <li>2021&2022, First Prize Scholarship of DLUT.</li>
  <li>2022, Champion on 3 tracks of Research Re-implementation Challenge by PaddlePaddle.</li>
  <li>2021, Top 1% on 2 tracks of Challenge of Xunfei AI Recognition Algorithm.</li>
  <li>2021, Top 1% on 3 tracks of Challenge of Guangdong Intelligent Recognition Algorithm.</li>
  <li>2020, Top 1.5% on Guangdong Industrial Intelligent Manufacturing Innovation Contest -Intelligent Algorithm Tournament.</li>
  <li>2020, Outstanding Graduate of Liaoning Province.</li>
  <li>2020, Outstanding Graduate of DLUT.</li>
  <li>2019, China Aerospace Science and Technology Corporation Scholarship.</li>
  <li>2018, Huawei Scholarship.</li>
  <li>2017, First Prize Scholarship of Lingshui.</li>
  <li>2017, Second Prize in Mathematics Competition of Dalian.</li>
  <li>2017&2018&2019, Outstanding Merit Student of DLUT.</li>
</ul>
</font>

<A NAME="Misc"><h2>Misc</h2></A>
<font size="3"> 
<ul>

  <li>Swimming (breaststroke). I won the 4th place in the final of DLUT swimming competition.</li>
  <li>Mountain climbing. Mountains and hills are everywhere in Dalian.</li>
  <li>Folk dance. I have participated in DLUT art performance (Fenglan Cup) for 3 times.</li>
</ul>
</font>


</div>
</div>

<p style="text-align: center; color: #8a8a8a;font-size: 14px;">
  Last updated date: <u>23 July, 2022</u>.
</p>

<p style="text-align: center; color: #8a8a8a;font-size: 12px;">
  Part of page is borrowed from <a href="https://jonbarron.info/" style="color: black;">Jon Barron</a> and <a href="https://yyysjz1997.github.io/" style="color: black;">Yiyuan Yang</a>.
</p>

<p style="text-align: center; color: #8a8a8a;font-size: 12px;">
  Copyright &copy2022-2023 Hongtao Wen. All Rights Reserved.
</p>

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
  $(function(){
      $(window).scroll(function(){  //If scroll
          var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
          if( scrollt >400 )
          {  
              $("#back_top").fadeIn(40); 
          }
          else
          {
              $("#back_top").stop().fadeOut(40);
          }
      });
  
      $("#back_top").click(function(){ 
  
          $("html,body").animate({scrollTop:"0px"}, 200);
  
      }); 
  
  });
  </script>

</body>
</html>
